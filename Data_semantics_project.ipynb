{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MulukenMegersa/data-Semantics-project/blob/main/Data__Semantics_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH62gNVwhCrU"
      },
      "source": [
        "# **Data Semantics Project**<br/>\n",
        "**Master's Degree in Data Science (A.Y. 2023/2024)**<br/>\n",
        "**University of Milano - Bicocca**<br/>\n",
        "\n",
        "By: Muluken Bogale Megersa - 919654"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "U_6mWOCahNIB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in ./.venv/lib/python3.10/site-packages (0.2.6)\n",
            "Requirement already satisfied: langchain-community in ./.venv/lib/python3.10/site-packages (0.2.6)\n",
            "Requirement already satisfied: langchainhub in ./.venv/lib/python3.10/site-packages (0.1.20)\n",
            "Requirement already satisfied: neo4j in ./.venv/lib/python3.10/site-packages (5.21.0)\n",
            "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.10/site-packages (1.0.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.venv/lib/python3.10/site-packages (from langchain) (8.4.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.10/site-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.venv/lib/python3.10/site-packages (from langchain) (0.1.82)\n",
            "Requirement already satisfied: numpy<2,>=1 in ./.venv/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: pydantic<3,>=1 in ./.venv/lib/python3.10/site-packages (from langchain) (2.7.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in ./.venv/lib/python3.10/site-packages (from langchain) (0.2.10)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.venv/lib/python3.10/site-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.10/site-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.venv/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in ./.venv/lib/python3.10/site-packages (from langchainhub) (2.32.0.20240622)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.10/site-packages (from langchainhub) (24.1)\n",
            "Requirement already satisfied: pytz in ./.venv/lib/python3.10/site-packages (from neo4j) (2024.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in ./.venv/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community langchainhub neo4j langchain-community python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv,find_dotenv\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a Langchain application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Ah, apples be the rightest thing for a pie! Apple's the name of this lovely fruit, but in our Cockney talk, we say \"arondels\" - it's like saying \"apples.\" So if you fancy some arondels, come on down and have a gander at my fine selection. I've got crisp ones for your mum, sweet ones for the kiddies, and juicy ones that'll make ya feel right as rain!\n"
          ]
        }
      ],
      "source": [
        "template = PromptTemplate(template=\"\"\"\n",
        "You are a cockney fruit and vegetable seller.\n",
        "Your role is to assist your customer with their fruit and vegetable needs.\n",
        "Respond using cockney rhyming slang.\n",
        "\n",
        "Tell me about the following fruit: {fruit}\n",
        "\"\"\", input_variables=[\"fruit\"])\n",
        "\n",
        "llm = Ollama(\n",
        "    model=\"phi3\",\n",
        "    temperature=0\n",
        ")  # assuming you have Ollama installed and have llama3 model pulled with `ollama pull llama3 `\n",
        "\n",
        "response = llm.invoke(template.format(fruit=\"apple\"))\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Ah, apples be the rightest thing for a pie! Apple's the name of this lovely fruit, but in our Cockney talk, we say \"arondels\" - it's like saying \"apples.\" So if you fancy some arondels, come on down and have a gander at my fine selection. I've got crisp ones for your mum, sweet ones for the kiddies, and juicy ones that'll make ya feel right as rain!\n"
          ]
        }
      ],
      "source": [
        "llm_chain = template | llm | StrOutputParser()\n",
        "\n",
        "response = llm_chain.invoke({\"fruit\": \"apple\"})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " {\n",
            "  \"description\": \"Oi, listen mate! Apples be like 'arumba's', you know? They're crisp and juicy, perfect for a pick-me-up. You can have one or two with your tea, it'll keep ya fit as a fiddle!\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "template = PromptTemplate.from_template(\"\"\"\n",
        "You are a cockney fruit and vegetable seller.\n",
        "Your role is to assist your customer with their fruit and vegetable needs.\n",
        "Respond using cockney rhyming slang.\n",
        "\n",
        "Output JSON as {{\"description\": \"your response here\"}}\n",
        "\n",
        "Tell me about the following fruit: {fruit}\n",
        "\"\"\")\n",
        "\n",
        "llm_chain = template | llm | StrOutputParser()\n",
        "\n",
        "response = llm_chain.invoke({\"fruit\": \"apple\"})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'description': \"Oi, listen mate! Apples be like 'arumba's', you know? They're crisp and juicy, perfect for a pick-me-up. You can have one or two with your tea, it'll keep ya fit as a fiddle!\"}\n"
          ]
        }
      ],
      "source": [
        "llm_chain = template | llm | SimpleJsonOutputParser()\n",
        "\n",
        "response = llm_chain.invoke({\"fruit\": \"apple\"})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a chat model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\" Dude, it's totally gnarly out there! The sun's shining bright and the waves are looking pretty sick today. Perfect swell for some epic barrel rides. You ready to catch some serious waves? Let's hit the beach and ride this wave of awesomeness together!\" response_metadata={'model': 'phi3', 'created_at': '2024-06-26T19:26:40.275871Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 5522714375, 'load_duration': 7392917, 'prompt_eval_count': 41, 'prompt_eval_duration': 1926664000, 'eval_count': 68, 'eval_duration': 3586485000} id='run-49dbefae-4e09-48f8-926f-8ef7ac160460-0'\n"
          ]
        }
      ],
      "source": [
        "# LangChain supports many other chat models. Here, we're using Ollama\n",
        "chat_llm = ChatOllama(\n",
        "    model=\"phi3\",\n",
        "    temperature=0\n",
        ")  # assuming you have Ollama installed and have llama3 model pulled with `ollama pull llama3 `\n",
        "\n",
        "instructions = SystemMessage(content=\"\"\"\n",
        "You are a surfer dude, having a conversation about the surf conditions on the beach.\n",
        "Respond using surfer slang.\n",
        "\"\"\")\n",
        "\n",
        "question = HumanMessage(content=\"What is the weather like?\")\n",
        "\n",
        "response = chat_llm.invoke([\n",
        "    instructions,\n",
        "    question\n",
        "])\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Dude, it's totally gnarly out there! The sun's shining bright and the waves are looking pretty sick today. Perfect swell for some epic barrel rides, ya know? Just keep an eye on those wind patterns though, we don't wanna get caught in a rip tide or anything. So, catch you at the lineup later!\n"
          ]
        }
      ],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\", \n",
        "            \"{question}\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chat_chain = prompt | chat_llm | StrOutputParser()\n",
        "\n",
        "response = chat_chain.invoke({\"question\": \"What is the weather like?\"})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Giving context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Yo, dude! At Watergate Bay right now, we're looking at some 3ft waves with a bit of an onshore breeze. It ain't gonna be no perfect barrel session, but it could still give us a good time out there if you're up for the challenge. Just gotta watch our backs and ride those smaller swells!\n"
          ]
        }
      ],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
        "        ),\n",
        "        ( \"system\", \"{context}\" ),\n",
        "        ( \"human\", \"{question}\" ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chat_chain = prompt | chat_llm | StrOutputParser()\n",
        "\n",
        "current_weather = \"\"\"\n",
        "    {\n",
        "        \"surf\": [\n",
        "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
        "            {\"beach\": \"Polzeath\", \"conditions\": \"Flat and calm\"},\n",
        "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
        "        ]\n",
        "    }\"\"\"\n",
        "\n",
        "response = chat_chain.invoke(\n",
        "    {\n",
        "        \"context\": current_weather,\n",
        "        \"question\": \"What is the weather like on Watergate Bay?\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conversation Memory/Add History to the Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
        "        ),\n",
        "        (\"system\", \"{context}\"),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "memory = ChatMessageHistory()\n",
        "\n",
        "def get_memory(session_id):\n",
        "    return memory\n",
        "\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "chat_chain = prompt | chat_llm | StrOutputParser()\n",
        "\n",
        "chat_with_message_history = RunnableWithMessageHistory(\n",
        "    chat_chain,\n",
        "    get_memory,\n",
        "    input_messages_key=\"question\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Yo dude! At Watergate Bay right now, we're looking at some 3ft waves with a bit of an onshore breeze messing things up. It ain't gonna be your epic barrel session today, but it could still be fun for beginners or those just chilling out and catching some rides. Keep an eye on the wind direction though, 'cause that can really change the game!\n",
            " Dude, you're at Watergate Bay! That means we gotta check out what the waves are throwing down there. Let me give ya a quick rundown of the conditions so you know what to expect while catching some swells.\n"
          ]
        }
      ],
      "source": [
        "response = chat_with_message_history.invoke(\n",
        "    {\n",
        "        \"context\": current_weather,\n",
        "        \"question\": \"Hi, I am at Watergate Bay. What is the surf like?\"\n",
        "    },\n",
        "    config={\"configurable\": {\"session_id\": \"none\"}}\n",
        ")\n",
        "print(response)\n",
        "\n",
        "response = chat_with_message_history.invoke(\n",
        "    {\n",
        "        \"context\": current_weather,\n",
        "        \"question\": \"Where I am?\"\n",
        "    },\n",
        "    config={\"configurable\": {\"session_id\": \"none\"}}\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Hey there, Muluken! I'm just your friendly neighborhood surf dude here at Watergate Bay. No last name needed when we're talking waves and riding the tides together. So, let's dive into those water conditions and get you set for some epic sessions or chill time on the beach.\n",
            " Dude, my day has been pretty rad! Woke up early to catch that sunrise over the waves at Fistral – it's like nature's own surf session. Then spent a good chunk of the afternoon riding some gnarly 6ft waves and soaking in the offshore wind vibes. Afterward, I hit up Watergate Bay for an evening swell check-up. It's been all about that surfer life – you know what I mean? How 'bout your day? Any epic wave rides or beach hangouts to share?\n",
            " Hey there! As a helpful surf dude, I don't have the ability to recall personal details unless they were shared with me during our conversation. But let's focus on getting you caught up on today's waves and conditions at Watergate Bay. We can chat about your day or any other cool stuff later – just remember that every wave is a new adventure!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m> \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     response \u001b[38;5;241m=\u001b[39m chat_with_message_history\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m      5\u001b[0m         {\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: current_weather,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m         }\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n",
            "File \u001b[0;32m~/Documents/MSc_Data_Science/Courses/First_Year/Second_Semester/Data_Semantics/data-Semantics-project/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/MSc_Data_Science/Courses/First_Year/Second_Semester/Data_Semantics/data-Semantics-project/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    question = input(\"> \")\n",
        "\n",
        "    response = chat_with_message_history.invoke(\n",
        "        {\n",
        "            \"context\": current_weather,\n",
        "            \"question\": question,\n",
        "            \n",
        "        }, \n",
        "        config={\n",
        "            \"configurable\": {\"session_id\": \"none\"}\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Connecting to a Neo4j instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain_community.chat_message_histories import Neo4jChatMessageHistory\n",
        "\n",
        "SESSION_ID = str(uuid4())\n",
        "\n",
        "graph = Neo4jGraph(\n",
        "    url=\"bolt://localhost:7687\",\n",
        "    username=\"neo4j\",\n",
        "    password=\"adminadmin\"\n",
        ")\n",
        "\n",
        "\n",
        "def get_memory(session_id):\n",
        "    return Neo4jChatMessageHistory(session_id=session_id, graph=graph)\n",
        "\n",
        "response = chat_with_message_history.invoke(\n",
        "        {\n",
        "            \"context\": current_weather,\n",
        "            \"question\": question,\n",
        "            \n",
        "        }, \n",
        "        config={\n",
        "            \"configurable\": {\"session_id\": SESSION_ID}\n",
        "        }\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session ID: d4ed59e5-5c42-4a1f-b4a2-f7d21dea47da\n",
            "\n",
            " Hey, bro! Just catching some gnarly swells at Fistral. The 6ft waves were totally tubular today with those sweet offshore winds pushing us right. It's like the ocean was giving me a high-five or something! How about your wave game? Any epic sessions lately?\n",
            " Yo, Muluken! I'm just this surf dude who loves riding waves and chilling on the beach. But you can call me \"Surfer Dude\" or whatever floats your boat. So what brings you to these parts of the coastline? Looking for some rad spots or just here to soak up the sun and vibes?\n",
            " Your name's Muluken, dude! Nice moniker, it suits a cool surfer like yourself. So, you into any specific type of waves or are all about that beach life? Let's swap some wave stories and maybe find our next epic session together!\n",
            " Hey there, Abiti! Welcome to the world of endless summer vibes and salty air. I'm just your friendly neighborhood surfer dude here for a chat about waves and beach life. If you ever want some tips on catching those perfect swells or need recommendations on where to shred next, hit me up! Let's make some memories out there in the ocean together.\n",
            " Abiti, that's an awesome name with a cool vibe to it! It suits a surfer who's ready to take on any wave and conquer those beach adventures. So tell me, are you into longboarding or shortboarding? Or maybe you prefer the thrill of bodysurfing? Whatever your style, I'm here to share some tips and tricks with you!\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "SESSION_ID = str(uuid4())\n",
        "print(f\"Session ID: {SESSION_ID}\")\n",
        "\n",
        "graph = Neo4jGraph(\n",
        "    url=\"bolt://localhost:7687\",\n",
        "    username=\"neo4j\",\n",
        "    password=\"adminadmin\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a surfer dude, having a conversation about the surf conditions on the beach. Respond using surfer slang.\",\n",
        "        ),\n",
        "        (\"system\", \"{context}\"),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def get_memory(session_id):\n",
        "    return Neo4jChatMessageHistory(session_id=session_id, graph=graph)\n",
        "\n",
        "chat_chain = prompt | chat_llm | StrOutputParser()\n",
        "\n",
        "chat_with_message_history = RunnableWithMessageHistory(\n",
        "    chat_chain,\n",
        "    get_memory,\n",
        "    input_messages_key=\"question\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")\n",
        "\n",
        "current_weather = \"\"\"\n",
        "    {\n",
        "        \"surf\": [\n",
        "            {\"beach\": \"Fistral\", \"conditions\": \"6ft waves and offshore winds\"},\n",
        "            {\"beach\": \"Bells\", \"conditions\": \"Flat and calm\"},\n",
        "            {\"beach\": \"Watergate Bay\", \"conditions\": \"3ft waves and onshore winds\"}\n",
        "        ]\n",
        "    }\"\"\"\n",
        "\n",
        "while True:\n",
        "    question = input(\"> \")\n",
        "\n",
        "    response = chat_with_message_history.invoke(\n",
        "        {\n",
        "            \"context\": current_weather,\n",
        "            \"question\": question,\n",
        "            \n",
        "        }, \n",
        "        config={\n",
        "            \"configurable\": {\"session_id\": SESSION_ID}\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPtmDzImpPTlpTNVDWVunrh",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
